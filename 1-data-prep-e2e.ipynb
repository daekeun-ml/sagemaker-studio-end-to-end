{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Data Preparation, Process, and Store Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='all-up-overview'></a>\n",
    "\n",
    "## [Overview](./0-AutoClaimFraudDetection.ipynb)\n",
    "* [Notebook 0: Overview, Architecture and Data Exploration](./0-AutoClaimFraudDetection.ipynb)\n",
    "* **[Notebook 1: Data Preparation, Process, and Store Features](./1-data-prep-e2e.ipynb)**\n",
    "  * **[Architecture](#arch)**\n",
    "  * **[Getting started](#aud-getting-started)**\n",
    "  * **[DataSets](#aud-datasets)**\n",
    "  * **[SageMaker Feature Store](#aud-feature-store)**\n",
    "  * **[Create train and test datasets](#aud-dataset)**\n",
    "* [Notebook 2: Train, Check Bias, Tune, Record Lineage, and Register a Model](./2-lineage-train-assess-bias-tune-registry-e2e.ipynb)\n",
    "* [Notebook 3: Mitigate Bias, Train New Model, Store in Registry](./3-mitigate-bias-train-model2-registry-e2e.ipynb)\n",
    "* [Notebook 4: Deploy Model, Run Predictions](./4-deploy-run-inference-e2e.ipynb)\n",
    "* [Notebook 5: Create and Run an End-to-End Pipeline to Deploy the Model](./5-pipeline-e2e.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 노트북의 목적은 ML 수명주기(lifecycle)의 데이터 준비 단계를 수행하는 것입니다. 주요 데이터 랭글링(data wrangling), 데이터 수집 및 다중 변환(multiple transformatio)이 SageMaker Studio Data Wrangler GUI를 통해 수행됩니다.\n",
    "\n",
    "이 노트북에서는 raw 데이터에 대한 변환을 정의하는 `.flow` 파일을 가져옵니다. `.csv` 파일로 S3 버킷에 저장된 raw 데이터에 이러한 변환을 적용하는 SageMaker Processing job을 사용하여 적용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arch'> </a>\n",
    "## Architecture for Data Prep, Process and Store Features\n",
    "[overview](#all-up-overview)\n",
    "___\n",
    "![Data Prep and Store](./images/e2e-1-pipeline-v3b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required and/or update third-party libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "install_needed = False\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !python -m pip install -Uq pip\n",
    "    !python -m pip install -q awscli==1.20.25 awswrangler==2.10.0 imbalanced-learn==0.8.0 sagemaker==2.54.0 boto3==1.18.25\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading stored variables\n",
    "\n",
    "이전에 이 노트북을 실행한 경우, AWS에서 생성한 리소스를 재사용할 수 있습니다. 아래 셀을 실행하여 이전에 생성된 변수를 로드합니다. 기존 변수의 출력물이 표시되어야 합니다. 인쇄된 내용이 보이지 않으면 노트북을 처음 실행한 것일 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Important</font>: StoreMagic 명령을 사용하여 변수를 검색하려면 이전 노트북을 실행해야 합니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import string\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aud-getting-started'></a>\n",
    "## Getting started: Creating Resources\n",
    "\n",
    "[overview](#all-up-overview)\n",
    "___\n",
    "\n",
    "이 노트북을 성공적으로 실행하려면, 몇 가지 AWS 리소스를 생성해야 합니다. 먼저 이 자습서의 모든 데이터를 저장하기 위해 S3 버킷이 생성됩니다. 생성된 후에는 IAM 콘솔을 사용하여 AWS Glue role을 생성한 다음, 이 노트북에 대한 FeatureStore 액세스를 허용하는 policy를 S3 버킷에 연결해야 합니다. 이미 이 노트북을 실행하고 중단한 부분을 선택하는 경우 아래 셀을 실행하면 추가 리소스를 생성하는 대신, 이미 생성한 리소스를 선택해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add FeatureStore policy to Studio's execution role\n",
    "\n",
    "![title](images/iam-policies.png)\n",
    "\n",
    "1. 별도의 브라우저 탭에서 AWS 콘솔의 IAM 섹션으로 이동합니다.\n",
    "2. Roles 섹션으로 이동하여 SageMaker Studio user에서 사용 중인 실행 role을 선택합니다.\n",
    "    * 어떤 role을 사용하고 있는지 확실하지 않은 경우, 아래 셀을 실행하여 출력하세요.\n",
    "3. 이 role에 <font color='green'> AmazonSageMakerFeatureStoreAccess </font> policy를 연결합니다. 연결되면 변경 사항이 즉시 적용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SageMaker Role:', sagemaker.get_execution_role().split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set region, boto3 and SageMaker SDK variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can change this to a region of your choice\n",
    "import sagemaker\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"Using AWS Region: {}\".format(region))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.setup_default_session(region_name=region)\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client('sagemaker')\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: SageMaker Studio 또는 SageMaker Classic 노트북에서 이 노트북을 실행하지 않는 경우, SageMakerFullAccess 및 SageMakerFeatureStoreFullAccess가 있는 AWS role로 sagemaker_execution_role_name을 인스턴스화해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_execution_role_name = 'AmazonSageMaker-ExecutionRole-20210107T234882'\n",
    "try:\n",
    "    sagemaker_role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    sagemaker_role = iam.get_role(RoleName=sagemaker_execution_role_name)['Role']['Arn']\n",
    "    print(f\"\\n instantiating sagemaker_role with supplied role name : {sagemaker_role}\")\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a directory in the SageMaker default bucket for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'bucket' not in locals():\n",
    "    bucket = sagemaker_session.default_bucket()\n",
    "    prefix = 'fraud-detect-demo'\n",
    "    %store bucket\n",
    "    %store prefix\n",
    "    print(f'Creating bucket: {bucket}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미 존재하는 자체 S3 버킷을 사용하려면 아래 코드 셀의 주석 처리를 제거하고 다음 예제 코드를 활용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "try:\n",
    "    s3_client.create_bucket(Bucket=bucket, ACL='private', CreateBucketConfiguration={'LocationConstraint': region})\n",
    "    print('Create S3 bucket: SUCCESS')\n",
    "    \n",
    "except Exception as e:\n",
    "    if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n",
    "        print(f'Using existing bucket: {bucket}/{prefix}')\n",
    "    else:\n",
    "        raise(e)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======> Tons of output_paths\n",
    "traing_job_output_path = f's3://{bucket}/{prefix}/training_jobs'\n",
    "bias_report_1_output_path = f's3://{bucket}/{prefix}/clarify-bias-1'\n",
    "bias_report_2_output_path = f's3://{bucket}/{prefix}/clarify-bias-2'\n",
    "explainability_output_path = f's3://{bucket}/{prefix}/clarify-explainability'\n",
    "\n",
    "train_data_uri = f's3://{bucket}/{prefix}/data/train/train.csv'\n",
    "test_data_uri = f's3://{bucket}/{prefix}/data/test/test.csv'\n",
    "\n",
    "#=======> variables used for parameterizing the notebook run\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "claify_instance_count = 1\n",
    "clairfy_instance_type = 'ml.c5.xlarge'\n",
    "\n",
    "predictor_instance_count = 1\n",
    "predictor_instance_type = \"ml.c5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload raw data to S3\n",
    "\n",
    "SageMaker Data Wrangler를 사용하여 raw 데이터를 전처리하려면, 데이터가 S3에 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='data/claims.csv', Bucket=bucket, Key=f'{prefix}/data/raw/claims.csv')\n",
    "s3_client.upload_file(Filename='data/customers.csv', Bucket=bucket, Key=f'{prefix}/data/raw/customers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update attributes within the  `.flow` file \n",
    "\n",
    "DataWrangler는 `.flow` 파일을 생성합니다. 여기에는 랭글링 중에 사용된 S3 버킷에 대한 참조가 포함됩니다. 이것은 이 노트북에 기본으로 설정되어 있는 것과 다를 수 있습니다. 예를 들어 다른 사람이 랭글링을 수행한 경우, 해당 버킷에 액세스할 수 없으므로 실제로 로드할 수 있도록 자신의 S3 버킷을 가리킴으로써 `.flow` 파일을 Wrangler에 저장하거나 데이터에 액세스할 수 있습니다.\n",
    "\n",
    "아래 셀을 실행한 후 `claim.flow` 및 `customers.flow` 파일을 열고 데이터를 S3로 내보내거나 제공된 `data/claims_preprocessed.csv` 및 `data/customers_preprocessed.csv` 파일을 사용하여 가이드를 계속할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_flow_template_file = \"claims_flow_template\"\n",
    "\n",
    "with open(claims_flow_template_file, 'r') as f:\n",
    "    variables   = {'bucket': bucket, 'prefix': prefix}\n",
    "    template    = string.Template(f.read())\n",
    "    claims_flow = template.substitute(variables)\n",
    "    claims_flow = json.loads(claims_flow)\n",
    "\n",
    "with open('claims.flow', 'w') as f:\n",
    "    json.dump(claims_flow, f)\n",
    "    \n",
    "customers_flow_template_file = \"customers_flow_template\"\n",
    "\n",
    "with open(customers_flow_template_file, 'r') as f:\n",
    "    variables      = {'bucket': bucket, 'prefix': prefix}\n",
    "    template       = string.Template(f.read())\n",
    "    customers_flow = template.substitute(variables)\n",
    "    customers_flow = json.loads(customers_flow)\n",
    "    \n",
    "with open('customers.flow', 'w') as f:\n",
    "    json.dump(customers_flow, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed data from Data Wrangler job\n",
    "\n",
    "`claim.flow` 및 `customers.flow` 에서 Data Wrangler job을 실행한 경우 여기에서 전처리된 데이터를 로드할 수 있습니다. Data Wrangler job을 실행하지 않은 경우에도 이 예제의 `/data` 디렉토리에서 미리 만들어진 데이터셋을 로드하여 시작할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aud-datasets'></a>\n",
    "## DataSets and Feature Types\n",
    "[overview](#all-up-overview)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_dtypes = {\n",
    "    \"policy_id\": int,\n",
    "    \"incident_severity\": int,\n",
    "    \"num_vehicles_involved\": int,\n",
    "    \"num_injuries\": int,\n",
    "    \"num_witnesses\": int,\n",
    "    \"police_report_available\": int,\n",
    "    \"injury_claim\": float,\n",
    "    \"vehicle_claim\": float,\n",
    "    \"total_claim_amount\": float,\n",
    "    \"incident_month\": int,\n",
    "    \"incident_day\": int,\n",
    "    \"incident_dow\": int,\n",
    "    \"incident_hour\": int,\n",
    "    \"fraud\": int,\n",
    "    \"driver_relationship_self\": int,\n",
    "    \"driver_relationship_na\": int,\n",
    "    \"driver_relationship_spouse\": int,\n",
    "    \"driver_relationship_child\": int,\n",
    "    \"driver_relationship_other\": int,\n",
    "    \"incident_type_collision\": int,\n",
    "    \"incident_type_breakin\": int,\n",
    "    \"incident_type_theft\": int,\n",
    "    \"collision_type_front\": int,\n",
    "    \"collision_type_rear\": int,\n",
    "    \"collision_type_side\": int,\n",
    "    \"collision_type_na\": int,\n",
    "    \"authorities_contacted_police\": int,\n",
    "    \"authorities_contacted_none\": int,\n",
    "    \"authorities_contacted_fire\": int,\n",
    "    \"authorities_contacted_ambulance\": int,\n",
    "    \"event_time\": float,\n",
    "}\n",
    "\n",
    "customers_dtypes = {\n",
    "    \"policy_id\": int,\n",
    "    \"customer_age\": int,\n",
    "    \"customer_education\": int,\n",
    "    \"months_as_customer\": int,\n",
    "    \"policy_deductable\": int,\n",
    "    \"policy_annual_premium\": int,\n",
    "    \"policy_liability\": int,\n",
    "    \"auto_year\": int,\n",
    "    \"num_claims_past_year\": int,\n",
    "    \"num_insurers_past_5_years\": int,\n",
    "    \"customer_gender_male\": int,\n",
    "    \"customer_gender_female\": int,\n",
    "    \"policy_state_ca\": int,\n",
    "    \"policy_state_wa\": int,\n",
    "    \"policy_state_az\": int,\n",
    "    \"policy_state_or\": int,\n",
    "    \"policy_state_nv\": int,\n",
    "    \"policy_state_id\": int,\n",
    "    \"event_time\": float,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======> This is your DataFlow output path if you decide to redo the work in DataFlow on your own\n",
    "flow_output_path = 'YOUR_PATH_HERE'\n",
    "\n",
    "try:\n",
    "    # this will try to load the exported dataframes from the claims and customers .flow files\n",
    "    claims_s3_path = f'{flow_output_path}/claims_output'\n",
    "    customers_s3_path = f'{flow_output_path}/customers_output'\n",
    "    \n",
    "    claims_preprocessed = wr.s3.read_csv(\n",
    "        path=claims_s3_path, \n",
    "        dataset=True, \n",
    "        index_col=0, \n",
    "        dtype=claims_dtypes)\n",
    "    \n",
    "    customers_preprocessed = wr.s3.read_csv(\n",
    "        path=customers_s3_path, \n",
    "        dataset=True, \n",
    "        index_col=0, \n",
    "        dtype=customers_dtypes)\n",
    "\n",
    "except:\n",
    "    # if the Data Wrangler job was not run, the claims and customers dataframes will be loaded from local copies\n",
    "    timestamp = pd.to_datetime('now').timestamp()\n",
    "    print('Unable to load Data Wrangler output. Loading pre-made dataframes...')\n",
    "    \n",
    "    claims_preprocessed = pd.read_csv(\n",
    "        filepath_or_buffer='data/claims_preprocessed.csv', \n",
    "        dtype=claims_dtypes)\n",
    "    \n",
    "    # a timestamp column is required by the feature store, so one is added with a current timestamp\n",
    "    claims_preprocessed['event_time'] = timestamp\n",
    "    \n",
    "    customers_preprocessed = pd.read_csv(\n",
    "        filepath_or_buffer='data/customers_preprocessed.csv', \n",
    "        dtype=customers_dtypes)\n",
    "    \n",
    "    customers_preprocessed['event_time'] = timestamp\n",
    "    \n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 올바른 데이터 유형과 함께 고객 및 청구 데이터를 포함하는 Pandas 데이터프레임이 있습니다. Dat Wrangler가 피쳐를 원-핫 인코딩 피쳐로 인코딩할 때, 결과 피쳐에 대한 데이터 유형을 float로 기본 설정합니다.\n",
    "\n",
    "<font color ='red'> Note: </font> : Data Wrangler에서 생성 된 범주형 피쳐에 대한 데이터 유형을 명시적으로 변환하는 이유는, Clarify에서 범주형 변수로 처리할 수 있도록 정수 유형인지 확인하기 위한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aud-feature-store'></a>\n",
    "## SageMaker Feature Store\n",
    "\n",
    "[overview](#all-up-overview)\n",
    "___\n",
    "\n",
    "Amazon SageMaker Feature Store는 피쳐를 저장하고 액세스할 수 있는 전용 리포지토리이므로, 팀 간에 이름을 지정하고 구성하고 재사용하기가 훨씬 쉽습니다. SageMaker Feature Store는 추가 코드를 작성하거나 피쳐를 일관되게 유지하기 위해 수동 프로세스를 생성할 필요 없이 훈련 및 실시간 추론 중 피쳐에 대한 통합 저장소를 제공합니다. SageMaker Feature Store는 저장된 피쳐의 메타데이터 (예: 피쳐 이름 또는 버전 번호)를 추적하므로 대화형 쿼리 서비스인 Amazon Athena를 사용하여 배치 또는 실시간으로 올바른 속성에 대한 피쳐를 쿼리할 수 있습니다. SageMaker Feature Store는 추론 중에 새 데이터가 생성될 때 단일 리포지토리가 업데이트되어 모델이 훈련 및 추론 중에 사용할 수 있도록 항상 새로운 피쳐를 사용할 수 있기 때문에 피쳐를 업데이트된 상태로 유지합니다.\n",
    "\n",
    "피쳐 저장소는 S3에 저장된 오프라인 컴포넌트와 지연 시간이 짧은 데이터베이스에 저장된 온라인 컴포넌트로 구성됩니다. 온라인 데이터베이스는 선택 사항이지만 추론에서 사용할 수 있는 추가 피쳐가 필요한 경우 매우 유용합니다. 이 섹션에서는 보험 청구 및 고객 데이터셋에 대한 피쳐 그룹을 생성합니다. 보험 청구 및 고객 데이터를 각 피쳐 그룹에 삽입한 후, Athena를 사용하여 오프라인 스토어를 쿼리하여 훈련 데이터 세트를 구축해야 합니다.\n",
    "\n",
    "SageMaker 피쳐 저장소에 대한 자세한 내용은 [SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html)를 참조해 주십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto_session.client(\n",
    "    service_name='sagemaker-featurestore-runtime', \n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the feature groups\n",
    "\n",
    "각 피쳐의 데이터 유형은 데이터프레임을 전달하고, 적절한 데이터 유형을 유추하여 설정됩니다. 피쳐 데이터 유형은 설정 변수(config variable)를 통해 설정할 수도 있지만, 피쳐 그룹에 수집될 때 Pandas 데이터프레임의 해당 Python 데이터 유형과 일치해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_fg_name = f'{prefix}-claims'\n",
    "customers_fg_name = f'{prefix}-customers'\n",
    "%store claims_fg_name \n",
    "%store customers_fg_name\n",
    "\n",
    "claims_feature_group = FeatureGroup(\n",
    "    name=claims_fg_name, \n",
    "    sagemaker_session=feature_store_session)\n",
    "\n",
    "customers_feature_group = FeatureGroup(\n",
    "    name=customers_fg_name, \n",
    "    sagemaker_session=feature_store_session)\n",
    "\n",
    "claims_feature_group.load_feature_definitions(data_frame=claims_preprocessed);\n",
    "customers_feature_group.load_feature_definitions(data_frame=customers_preprocessed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the feature groups\n",
    "데이터프레임에서 필요한 레코드 식별자 및 이벤트 타임 피쳐에 해당하는 컬럼을 피쳐 그룹에 알려야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{customers_fg_name} -- {claims_fg_name} are the feature group names in use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_feature_name = 'policy_id'\n",
    "event_time_feature_name = 'event_time'\n",
    "\n",
    "try:\n",
    "    print(f\"\\n Using s3://{bucket}/{prefix}\")\n",
    "    claims_feature_group.create(\n",
    "        s3_uri=f\"s3://{bucket}/{prefix}\",\n",
    "        record_identifier_name=record_identifier_feature_name,\n",
    "        event_time_feature_name=event_time_feature_name,\n",
    "        role_arn=sagemaker_role,\n",
    "        enable_online_store=True\n",
    "    )\n",
    "    print(f'Create \"claims\" feature group: SUCCESS')\n",
    "except Exception as e:\n",
    "    code = e.response.get('Error').get('Code')\n",
    "    if code == 'ResourceInUse':\n",
    "        print(f'Using existing feature group: {claims_fg_name}')\n",
    "    else:\n",
    "        raise(e)\n",
    "\n",
    "try:\n",
    "    customers_feature_group.create(\n",
    "        s3_uri=f\"s3://{bucket}/{prefix}\",\n",
    "        record_identifier_name=record_identifier_feature_name,\n",
    "        event_time_feature_name=event_time_feature_name,\n",
    "        role_arn=sagemaker_role,\n",
    "        enable_online_store=True\n",
    "    )\n",
    "    print(f'Create \"customers\" feature group: SUCCESS')\n",
    "except Exception as e:\n",
    "    code = e.response.get('Error').get('Code')\n",
    "    if code == 'ResourceInUse':\n",
    "        print(f'Using existing feature group: {customers_fg_name}')\n",
    "    else:\n",
    "        raise(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait until feature group creation has fully completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "    \n",
    "wait_for_feature_group_creation_complete(feature_group=claims_feature_group)\n",
    "wait_for_feature_group_creation_complete(feature_group=customers_feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest records into the Feature Groups\n",
    "\n",
    "피쳐 그룹이 생성된 후, PutRecord API를 사용하여 각 store에 데이터를 넣을 수 있습니다. 이 API는 높은 TPS를 처리할 수 있으며 다른 스트림에서 호출되도록 설계되었습니다. 이러한 모든 Put 요청의 데이터는 버퍼링되어 s3에 chunk로 기록됩니다. 수집 후 몇 분 이내에 파일이 오프라인 저장소에 기록됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'claims_table' in locals():\n",
    "    print(\"You may have already ingested the data into your Feature Groups. If you'd like to do this again, you can run the ingest methods outside of the 'if/else' statement.\")\n",
    "\n",
    "else:\n",
    "    claims_feature_group.ingest(data_frame=claims_preprocessed, max_workers=3, wait=True);\n",
    "    customers_feature_group.ingest(data_frame=customers_preprocessed, max_workers=3, wait=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for offline store data to become available\n",
    "아래 코드 셀은 약 5-8분이 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'claims_table' not in locals():\n",
    "    claims_table = (\n",
    "        claims_feature_group.describe()[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]\n",
    "    )\n",
    "if 'customers_table' not in locals():\n",
    "    customers_table = (\n",
    "        customers_feature_group.describe()[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]\n",
    "    )\n",
    "\n",
    "claims_feature_group_s3_prefix = (\n",
    "    f\"{prefix}/{account_id}/sagemaker/{region}/offline-store/{claims_table}/data\"\n",
    ")\n",
    "customers_feature_group_s3_prefix = (\n",
    "    f\"{prefix}/{account_id}/sagemaker/{region}/offline-store/{customers_table}/data\"\n",
    ")\n",
    "\n",
    "offline_store_contents = None\n",
    "while offline_store_contents is None:\n",
    "    objects_in_bucket = s3_client.list_objects(\n",
    "        Bucket=bucket, Prefix=customers_feature_group_s3_prefix\n",
    "    )\n",
    "    if \"Contents\" in objects_in_bucket and len(objects_in_bucket[\"Contents\"]) > 1:\n",
    "        offline_store_contents = objects_in_bucket[\"Contents\"]\n",
    "    else:\n",
    "        print(\"Waiting for data in offline store...\")\n",
    "        time.sleep(60)\n",
    "\n",
    "print(\"\\nData available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aud-dataset'></a>\n",
    "## Create train and test datasets\n",
    "\n",
    "[overview](#all-up-overview)\n",
    "___\n",
    "\n",
    "오프라인 스토어에서 데이터를 사용할 수있게 되면 자동으로 카탈로그화되고 Athena 테이블에 로드됩니다. (기본적으로 수행되지만 끌 수 있습니다.) 훈련 및 테스트 데이터셋을 구축하기 위해 Athena에서 생성된 Claims 및 Customers 테이블을 조인하는 SQL 쿼리를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_query = claims_feature_group.athena_query()\n",
    "customers_query = customers_feature_group.athena_query()\n",
    "\n",
    "claims_table = claims_query.table_name\n",
    "customers_table = customers_query.table_name\n",
    "database_name = customers_query.database\n",
    "%store claims_table\n",
    "%store customers_table\n",
    "%store database_name\n",
    "\n",
    "feature_columns = list(set(claims_preprocessed.columns) ^ set(customers_preprocessed.columns))\n",
    "feature_columns_string = \", \".join(f'\"{c}\"' for c in feature_columns)\n",
    "feature_columns_string = f'\"{claims_table}\".policy_id as policy_id, ' + feature_columns_string\n",
    "\n",
    "query_string = f\"\"\"\n",
    "SELECT DISTINCT {feature_columns_string}\n",
    "FROM \"{claims_table}\" LEFT JOIN \"{customers_table}\" \n",
    "ON \"{claims_table}\".policy_id = \"{customers_table}\".policy_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_query.run(query_string=query_string, output_location=f\"s3://{bucket}/{prefix}/query_results\")\n",
    "claims_query.wait()\n",
    "dataset = claims_query.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"./data/claims_customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = [\"fraud\"] + list(dataset.drop([\"fraud\", \"policy_id\"], axis=1).columns)\n",
    "%store col_order\n",
    "\n",
    "train = dataset.sample(frac=0.80, random_state=0)[col_order]\n",
    "test = dataset.drop(train.index)[col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write train, test data to S3\n",
    "\n",
    "훈련 데이터셋 및 테스트 데이터셋을 S3로 업로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"data/train.csv\", index=False)\n",
    "test.to_csv(\"data/test.csv\", index=False)\n",
    "dataset.to_csv(\"data/dataset.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='data/train.csv', Bucket=bucket, Key=f'{prefix}/data/train/train.csv')\n",
    "s3_client.upload_file(Filename='data/test.csv', Bucket=bucket, Key=f'{prefix}/data/test/test.csv')\n",
    "%store train_data_uri\n",
    "%store test_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Next Notebook: [Train, Check Bias, Tune, Record Lineage, Register Model](./2-lineage-train-assess-bias-tune-registry-e2e.ipynb)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
